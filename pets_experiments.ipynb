{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import omegaconf\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "os.environ['PATH'] += os.pathsep + os.pathsep.join([\"C:\\\\Users\\\\kehua\\\\.mujoco\\\\mujoco200\\\\bin;%PATH%\"])\n",
    "\n",
    "import mbrl.env.cartpole_continuous as cartpole_env\n",
    "import mbrl.env.cartpole_continuous_down as cartpole_env_down\n",
    "import mbrl.env.cartpole_continuous_travel as cartpole_env_travel\n",
    "# import mbrl.env.pets_halfcheetah as half_cheetah\n",
    "# import mbrl.env.pets_pusher as pusher\n",
    "# import mbrl.env.pets_reacher as reacher\n",
    "# import mbrl.env.humanoid_standup as humanoid_standup\n",
    "import mbrl.env.obstacles as obstacles\n",
    "\n",
    "import mbrl.env.reward_fns as reward_fns\n",
    "import mbrl.env.termination_fns as termination_fns\n",
    "import mbrl.models as models\n",
    "import mbrl.planning as planning\n",
    "import mbrl.util.common as common_util\n",
    "import mbrl.util as util\n",
    "from mbrl.types import TransitionBatch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import importlib\n",
    "importlib.reload(models)\n",
    "importlib.reload(cartpole_env_down)\n",
    "importlib.reload(planning)\n",
    "importlib.reload(common_util)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "mpl.rcParams.update({\"font.size\": 16})\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "env = cartpole_env_down.CartPoleEnvDown()\n",
    "env.seed(seed)\n",
    "env2 = cartpole_env_down.CartPoleEnvDown()\n",
    "env2.seed(seed)\n",
    "\n",
    "rng = np.random.default_rng(seed=0)\n",
    "generator = torch.Generator(device=device)\n",
    "generator.manual_seed(seed)\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape\n",
    "\n",
    "# This functions allows the model to evaluate the true rewards given an observation \n",
    "reward_fn = reward_fns.cartpole_down\n",
    "# This function allows the model to know if an observation should make the episode end\n",
    "term_fn = termination_fns.cartpole_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_length = 60\n",
    "num_trials = 8\n",
    "ensemble_size = 5\n",
    "n_delay = 1\n",
    "\n",
    "# Everything with \"???\" indicates an option with a missing value.\n",
    "# Our utility functions will fill in these details using the \n",
    "# environment information\n",
    "cfg_dict = {\n",
    "    # dynamics model configuration\n",
    "    \"dynamics_model\": {\n",
    "        \"model\": {\n",
    "            \"_target_\": \"mbrl.models.GaussianMLP\",\n",
    "            \"device\": device,\n",
    "            \"num_layers\": 3,\n",
    "            \"ensemble_size\": ensemble_size,\n",
    "            \"hid_size\": 200,\n",
    "            \"use_silu\": True,\n",
    "            \"in_size\": \"???\",\n",
    "            \"out_size\": \"???\",\n",
    "            \"deterministic\": False,\n",
    "            \"propagation_method\": \"fixed_model\"\n",
    "        }\n",
    "    },\n",
    "    # options for training the dynamics model\n",
    "    \"algorithm\": {\n",
    "        \"learned_rewards\": False,\n",
    "        \"target_is_delta\": True,\n",
    "        \"normalize\": True,\n",
    "    },\n",
    "    # these are experiment specific options\n",
    "    \"overrides\": {\n",
    "        \"trial_length\": trial_length,\n",
    "        \"num_steps\": num_trials * trial_length,\n",
    "        \"model_batch_size\": 32,\n",
    "        \"validation_ratio\": 0.05\n",
    "    }\n",
    "}\n",
    "cfg = omegaconf.OmegaConf.create(cfg_dict)\n",
    "\n",
    "agent_cfg = omegaconf.OmegaConf.create({\n",
    "    # this class evaluates many trajectories and picks the best one\n",
    "    \"_target_\": \"mbrl.planning.TrajectoryOptimizerAgent\",\n",
    "    \"planning_horizon\": 45,\n",
    "    \"replan_freq\": 1,\n",
    "    \"verbose\": False,\n",
    "    \"action_lb\": \"???\",\n",
    "    \"action_ub\": \"???\",\n",
    "    \"resample\": False,\n",
    "    # this is the optimizer to generate and choose a trajectory\n",
    "    \"optimizer_cfg\": {\n",
    "        \"_target_\": \"mbrl.planning.GradientOptimizer\",\n",
    "        \"num_iterations\": 5,\n",
    "        \"elite_ratio\": 0.1,\n",
    "        \"population_size\": 10,\n",
    "        \"alpha\": 0.1,\n",
    "        \"device\": device,\n",
    "        \"lower_bound\": \"???\",\n",
    "        \"upper_bound\": \"???\",\n",
    "        \"return_mean_elites\": True,\n",
    "        \"num_top\": 1,\n",
    "        \"resample_amount\": 10\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_scores = []\n",
    "\n",
    "def train_callback(_model, _total_calls, _epoch, tr_loss, val_score, _best_val):\n",
    "    train_losses.append(tr_loss)\n",
    "    val_scores.append(val_score.mean().item())   # this returns val score per ensemble model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dreward_total = {}\n",
    "for trial in range(num_trials):\n",
    "    dreward_total[trial] = []\n",
    "    \n",
    "values_total = {}\n",
    "for trial in range(num_trials):\n",
    "    values_total[trial] = []\n",
    "    \n",
    "values_prior = {}\n",
    "for trial in range(num_trials):\n",
    "    values_prior[trial] = []\n",
    "    \n",
    "values_total2 = {}\n",
    "for trial in range(num_trials):\n",
    "    values_total2[trial] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rew_all = []\n",
    "rew_all2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment: 0, trial: 0. Step 0\n",
      "experiment: 0, trial: 0. Step 1\n",
      "experiment: 0, trial: 0. Step 2\n",
      "experiment: 0, trial: 0. Step 3\n",
      "experiment: 0, trial: 0. Step 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e639dad04798>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[1;31m# --- Doing env step using the agent and adding to model dataset ---\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m             \u001b[0mnext_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcommon_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_env_and_add_to_buffer_optim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_env\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_opt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Caltech\\mbrl\\implementation test\\mbrl\\util\\common.py\u001b[0m in \u001b[0;36mstep_env_and_add_to_buffer_optim\u001b[1;34m(env, model_env, obs, agent, agent_kwargs, replay_buffer, callback, return_trajectory, use_opt, train)\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \"\"\"\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_opt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0magent_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m     \u001b[0mmodel_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_previous_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Caltech\\mbrl\\implementation test\\mbrl\\planning\\gradient_optimizer.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, obs, use_opt, **_kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_sequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m         \u001b[0mplan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward_fun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_fun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_opt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_opt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplan\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Caltech\\mbrl\\implementation test\\mbrl\\planning\\gradient_optimizer.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, reward_fun, cost_fun, use_opt, callback, trial_step)\u001b[0m\n\u001b[0;32m    374\u001b[0m                 \u001b[0mis_resampling\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m                 \u001b[0muse_opt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_opt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m                 \u001b[0mtrial_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m             )\n\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Caltech\\mbrl\\implementation test\\mbrl\\planning\\gradient_optimizer.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, reward_fun, obj_fun, x0, default_x0, callback, is_resampling, use_opt, trial_step)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0muse_opt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m             \u001b[0mtop_trajectories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize_trajectory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj_fun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_trajectories\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[0mtrajectory_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_trajectories\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Caltech\\mbrl\\implementation test\\mbrl\\planning\\gradient_optimizer.py\u001b[0m in \u001b[0;36moptimize_trajectory_batch\u001b[1;34m(self, obj_fun, action_sequences_list, start_lr, factor_shrink, max_tries, max_iterations)\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;31m# Compute objectives of all trajectories after stepping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[0maction_sequences_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_sequences_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m             \u001b[0mobjective_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_sequences_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mbackwards_pass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Caltech\\mbrl\\implementation test\\mbrl\\planning\\gradient_optimizer.py\u001b[0m in \u001b[0;36mcost_fun\u001b[1;34m(action_sequences, use_grad)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcost_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_sequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_sequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[0mplan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward_fun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_fun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_opt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_opt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Caltech\\mbrl\\implementation test\\mbrl\\planning\\gradient_optimizer.py\u001b[0m in \u001b[0;36mcost_fun\u001b[1;34m(initial_state, action_sequences, use_grad)\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcost_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_sequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m         return model_env.compute_objective(initial_state, action_sequences, \n\u001b[1;32m--> 553\u001b[1;33m         \u001b[0mnum_particles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m         )\n\u001b[0;32m    555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Caltech\\mbrl\\implementation test\\mbrl\\models\\model_env.py\u001b[0m in \u001b[0;36mcompute_objective\u001b[1;34m(self, initial_state, action_sequences, num_particles, use_grad)\u001b[0m\n\u001b[0;32m    267\u001b[0m         _, total_rewards = self.run_trajectory(initial_state, action_sequences, \n\u001b[0;32m    268\u001b[0m             \u001b[0mnum_particles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m             shuffle_indices=False)\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[0mtotal_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mtotal_rewards\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Caltech\\mbrl\\implementation test\\mbrl\\models\\model_env.py\u001b[0m in \u001b[0;36mrun_trajectory\u001b[1;34m(self, initial_state, action_sequences, num_particles, use_grad, sample, shuffle_indices)\u001b[0m\n\u001b[0;32m    315\u001b[0m             )\n\u001b[0;32m    316\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0muse_grad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m                 \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_particles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m                 \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_particles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Caltech\\mbrl\\implementation test\\mbrl\\models\\model_env.py\u001b[0m in \u001b[0;36mstep_grad\u001b[1;34m(self, actions, sample, num_particles)\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[0mpred_rewards\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_observs_mean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m         )\n\u001b[0;32m    218\u001b[0m         \u001b[0mdones\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtermination_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_observs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Caltech\\mbrl\\implementation test\\mbrl\\env\\reward_fns.py\u001b[0m in \u001b[0;36mcartpole_down\u001b[1;34m(act, next_obs)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mposition_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     return ((theta_reward*position_reward)\n\u001b[0m\u001b[0;32m     27\u001b[0m         * ~termination_fns.cartpole_down(act, next_obs))\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzoAAADvCAYAAAA6qqG1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAARbElEQVR4nO3db4il53ke8Ou2NkqI49glu4GgXUUqXddZTMDuoLoEGge7ZaUPu1/cIIFJHIQX0iqFxgQUUpygfKpDMQTUOtvGOAnEsuIPyRI2qJAouITIaIwbYckItopjDQpo47j6YmxF7d0PZ2zGZ8678+7ozB89+/vBwnnPeXTm9sPMXL7mfc851d0BAAAYyZuOegAAAIB1U3QAAIDhKDoAAMBwFB0AAGA4ig4AADAcRQcAABjOnkWnqj5ZVS9X1ZcmHq+q+s2qulZVz1TVu9c/JgCsJqcAWGXOGZ1PJTl/g8fvTXJ2+9+lJP/19Y8FALN9KnIKgCV7Fp3u/lySv7/BkotJfrcXnkrytqr6kXUNCAA3IqcAWGUdr9G5I8mLO463tu8DgONATgHcgk6s4TlqxX29cmHVpSwuG8ib3/zmf/aOd7xjDV8egP36whe+8Hfdfeqo5zhgcgrgDer15NQ6is5WkjM7jk8neWnVwu6+nORykmxsbPTm5uYavjwA+1VVf3PUMxwCOQXwBvV6cmodl65dSfIz2+9q854kr3T3367heQFgHeQUwC1ozzM6VfXpJO9NcrKqtpL8apLvSZLu/kSSq0nuS3ItyTeS/NxBDQsAy+QUAKvsWXS6+4E9Hu8k/25tEwHATZBTAKyyjkvXAAAAjhVFBwAAGI6iAwAADEfRAQAAhqPoAAAAw1F0AACA4Sg6AADAcBQdAABgOIoOAAAwHEUHAAAYjqIDAAAMR9EBAACGo+gAAADDUXQAAIDhKDoAAMBwFB0AAGA4ig4AADAcRQcAABiOogMAAAxH0QEAAIaj6AAAAMNRdAAAgOEoOgAAwHAUHQAAYDiKDgAAMBxFBwAAGI6iAwAADEfRAQAAhqPoAAAAw1F0AACA4Sg6AADAcBQdAABgOLOKTlWdr6rnq+paVT284vE7q+rJqvpiVT1TVfetf1QAWE1OAbBsz6JTVbcleTTJvUnOJXmgqs4tLfuPSR7v7ncluT/Jf1n3oACwipwCYJU5Z3TuSXKtu1/o7leTPJbk4tKaTvKD27ffmuSl9Y0IADckpwDY5cSMNXckeXHH8VaSf7605teS/I+q+oUkb07y/rVMBwB7k1MA7DLnjE6tuK+Xjh9I8qnuPp3kviS/V1W7nruqLlXVZlVtXr9+/eanBYDd5BQAu8wpOltJzuw4Pp3dp/wfTPJ4knT3Xyb5viQnl5+ouy9390Z3b5w6dWp/EwPAd5NTAOwyp+g8neRsVd1dVbdn8SLOK0trvprkfUlSVT+WRYD4UxgAh0FOAbDLnkWnu19L8lCSJ5J8OYt3rXm2qh6pqgvbyz6S5MNV9VdJPp3kQ929fNkAAKydnAJglTlvRpDuvprk6tJ9H91x+7kkP7He0QBgHjkFwLJZHxgKAADwRqLoAAAAw1F0AACA4Sg6AADAcBQdAABgOIoOAAAwHEUHAAAYjqIDAAAMR9EBAACGo+gAAADDUXQAAIDhKDoAAMBwFB0AAGA4ig4AADAcRQcAABiOogMAAAxH0QEAAIaj6AAAAMNRdAAAgOEoOgAAwHAUHQAAYDiKDgAAMBxFBwAAGI6iAwAADEfRAQAAhqPoAAAAw1F0AACA4Sg6AADAcBQdAABgOIoOAAAwHEUHAAAYjqIDAAAMZ1bRqarzVfV8VV2rqocn1vx0VT1XVc9W1e+vd0wAmCanAFh2Yq8FVXVbkkeT/KskW0merqor3f3cjjVnk/xykp/o7q9X1Q8f1MAAsJOcAmCVOWd07klyrbtf6O5XkzyW5OLSmg8nebS7v54k3f3yescEgElyCoBd5hSdO5K8uON4a/u+nd6e5O1V9RdV9VRVnV/1RFV1qao2q2rz+vXr+5sYAL6bnAJglzlFp1bc10vHJ5KcTfLeJA8k+e9V9bZd/1H35e7e6O6NU6dO3eysALCKnAJglzlFZyvJmR3Hp5O8tGLNH3X3P3T3Xyd5PotAAYCDJqcA2GVO0Xk6ydmquruqbk9yf5IrS2v+MMlPJUlVncziEoEX1jkoAEyQUwDssmfR6e7XkjyU5IkkX07yeHc/W1WPVNWF7WVPJPlaVT2X5Mkkv9TdXzuooQHg2+QUAKtU9/JlzIdjY2OjNzc3j+RrA7BQVV/o7o2jnuM4klMAR+/15NSsDwwFAAB4I1F0AACA4Sg6AADAcBQdAABgOIoOAAAwHEUHAAAYjqIDAAAMR9EBAACGo+gAAADDUXQAAIDhKDoAAMBwFB0AAGA4ig4AADAcRQcAABiOogMAAAxH0QEAAIaj6AAAAMNRdAAAgOEoOgAAwHAUHQAAYDiKDgAAMBxFBwAAGI6iAwAADEfRAQAAhqPoAAAAw1F0AACA4Sg6AADAcBQdAABgOIoOAAAwHEUHAAAYjqIDAAAMR9EBAACGM6voVNX5qnq+qq5V1cM3WPeBquqq2ljfiABwY3IKgGV7Fp2qui3Jo0nuTXIuyQNVdW7Furck+fdJPr/uIQFgipwCYJU5Z3TuSXKtu1/o7leTPJbk4op1v57kY0m+ucb5AGAvcgqAXeYUnTuSvLjjeGv7vu+oqnclOdPdf3yjJ6qqS1W1WVWb169fv+lhAWAFOQXALnOKTq24r7/zYNWbknw8yUf2eqLuvtzdG929cerUqflTAsA0OQXALnOKzlaSMzuOTyd5acfxW5K8M8mfV9VXkrwnyRUv9ATgkMgpAHaZU3SeTnK2qu6uqtuT3J/kyrcf7O5Xuvtkd9/V3XcleSrJhe7ePJCJAeC7ySkAdtmz6HT3a0keSvJEki8neby7n62qR6rqwkEPCAA3IqcAWOXEnEXdfTXJ1aX7Pjqx9r2vfywAmE9OAbBs1geGAgAAvJEoOgAAwHAUHQAAYDiKDgAAMBxFBwAAGI6iAwAADEfRAQAAhqPoAAAAw1F0AACA4Sg6AADAcBQdAABgOIoOAAAwHEUHAAAYjqIDAAAMR9EBAACGo+gAAADDUXQAAIDhKDoAAMBwFB0AAGA4ig4AADAcRQcAABiOogMAAAxH0QEAAIaj6AAAAMNRdAAAgOEoOgAAwHAUHQAAYDiKDgAAMBxFBwAAGI6iAwAADEfRAQAAhjOr6FTV+ap6vqquVdXDKx7/xap6rqqeqao/raofXf+oALCanAJg2Z5Fp6puS/JoknuTnEvyQFWdW1r2xSQb3f3jST6b5GPrHhQAVpFTAKwy54zOPUmudfcL3f1qkseSXNy5oLuf7O5vbB8+leT0escEgElyCoBd5hSdO5K8uON4a/u+KQ8m+ZPXMxQA3AQ5BcAuJ2asqRX39cqFVR9MspHkJycev5TkUpLceeedM0cEgBuSUwDsMueMzlaSMzuOTyd5aXlRVb0/ya8kudDd31r1RN19ubs3unvj1KlT+5kXAJbJKQB2mVN0nk5ytqrurqrbk9yf5MrOBVX1riS/lUV4vLz+MQFgkpwCYJc9i053v5bkoSRPJPlykse7+9mqeqSqLmwv+40kP5DkD6rqf1XVlYmnA4C1klMArDLnNTrp7qtJri7d99Edt9+/5rkAYDY5BcCyWR8YCgAA8Eai6AAAAMNRdAAAgOEoOgAAwHAUHQAAYDiKDgAAMBxFBwAAGI6iAwAADEfRAQAAhqPoAAAAw1F0AACA4Sg6AADAcBQdAABgOIoOAAAwHEUHAAAYjqIDAAAMR9EBAACGo+gAAADDUXQAAIDhKDoAAMBwFB0AAGA4ig4AADAcRQcAABiOogMAAAxH0QEAAIaj6AAAAMNRdAAAgOEoOgAAwHAUHQAAYDiKDgAAMBxFBwAAGI6iAwAADGdW0amq81X1fFVdq6qHVzz+vVX1me3HP19Vd617UACYIqcAWLZn0amq25I8muTeJOeSPFBV55aWPZjk6939T5J8PMl/WvegALCKnAJglTlndO5Jcq27X+juV5M8luTi0pqLSX5n+/Znk7yvqmp9YwLAJDkFwC5zis4dSV7ccby1fd/KNd39WpJXkvzQOgYEgD3IKQB2OTFjzaq/ePU+1qSqLiW5tH34rar60oyvfys6meTvjnqIY8reTLM30+zNtH961AOsgZw6fH6mptmbafZmmr2Ztu+cmlN0tpKc2XF8OslLE2u2qupEkrcm+fvlJ+ruy0kuJ0lVbXb3xn6GHp29mWZvptmbafZmWlVtHvUMayCnDpm9mWZvptmbafZm2uvJqTmXrj2d5GxV3V1Vtye5P8mVpTVXkvzs9u0PJPmz7t71lzIAOAByCoBd9jyj092vVdVDSZ5IcluST3b3s1X1SJLN7r6S5LeT/F5VXcviL2T3H+TQAPBtcgqAVeZcupbuvprk6tJ9H91x+5tJ/s1Nfu3LN7n+VmJvptmbafZmmr2ZNsTeyKlDZ2+m2Ztp9maavZm2770pZ+4BAIDRzHmNDgAAwBvKgRedqjpfVc9X1bWqenjF499bVZ/ZfvzzVXXXQc90XMzYm1+squeq6pmq+tOq+tGjmPMo7LU3O9Z9oKq6qm6ZdyqZszdV9dPb3zvPVtXvH/aMR2XGz9SdVfVkVX1x++fqvqOY87BV1Ser6uWpt0quhd/c3rdnqurdhz3jUZJT0+TUNDk1TU5Nk1OrHVhOdfeB/cviRaH/O8k/TnJ7kr9Kcm5pzb9N8ont2/cn+cxBznRc/s3cm59K8v3bt3/e3uxa95Ykn0vyVJKNo577uOxNkrNJvpjkH20f//BRz32M9uZykp/fvn0uyVeOeu5D2pt/meTdSb408fh9Sf4ki8+aeU+Szx/1zMfs+0ZOyamb3pvtdXJKTt3s3sip1Y/vK6cO+ozOPUmudfcL3f1qkseSXFxaczHJ72zf/myS91XVqg92G82ee9PdT3b3N7YPn8risyFuBXO+b5Lk15N8LMk3D3O4IzZnbz6c5NHu/nqSdPfLhzzjUZmzN53kB7dvvzW7P2tlSN39uaz4zJgdLib53V54KsnbqupHDme6IyenpsmpaXJqmpyaJqcmHFROHXTRuSPJizuOt7bvW7mmu19L8kqSHzrguY6DOXuz04NZNNlbwZ57U1XvSnKmu//4MAc7BuZ837w9ydur6i+q6qmqOn9o0x2tOXvza0k+WFVbWbxD1y8czmjH3s3+PhqJnJomp6bJqWlyapqc2r995dSst5d+HVb9xWv5bd7mrBnR7P/dVfXBJBtJfvJAJzo+brg3VfWmJB9P8qHDGugYmfN9cyKLywLem8VfV/9nVb2zu//PAc921ObszQNJPtXd/7mq/kUWn6vyzu7+fwc/3rF2q/4eTuTUjcipaXJqmpyaJqf2b1+/hw/6jM5WkjM7jk9n9ym476ypqhNZnKa70amrUczZm1TV+5P8SpIL3f2tQ5rtqO21N29J8s4kf15VX8niWs0rt8gLPef+TP1Rd/9Dd/91kuezCJTRzdmbB5M8niTd/ZdJvi/JyUOZ7nib9ftoUHJqmpyaJqemyalpcmr/9pVTB110nk5ytqrurqrbs3gR55WlNVeS/Oz27Q8k+bPeftXR4Pbcm+3T3r+VRXjcKtevJnvsTXe/0t0nu/uu7r4ri+vCL3T35tGMe6jm/Ez9YRYvEE5VncziEoEXDnXKozFnb76a5H1JUlU/lkWAXD/UKY+nK0l+Zvtdbd6T5JXu/tujHuqQyKlpcmqanJomp6bJqf3bV04d6KVr3f1aVT2U5Iks3mnik939bFU9kmSzu68k+e0sTstdy+IvZPcf5EzHxcy9+Y0kP5DkD7Zf9/rV7r5wZEMfkpl7c0uauTdPJPnXVfVckv+b5Je6+2tHN/XhmLk3H0ny36rqP2RxyvtDt8L/Ya2qT2dxicjJ7eu+fzXJ9yRJd38ii+vA70tyLck3kvzc0Ux6+OTUNDk1TU5Nk1PT5NS0g8qpugX2DgAAuMUc+AeGAgAAHDZFBwAAGI6iAwAADEfRAQAAhqPoAAAAw1F0AACA4Sg6AADAcBQdAABgOP8fLmWZit2thsgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1008x270 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_experiments = 20\n",
    "for experiment_number in range(num_experiments):\n",
    "    # Create a 1-D dynamics model for this environme|nt\n",
    "    dynamics_model = common_util.create_one_dim_tr_model(cfg, obs_shape, act_shape, n_delay=n_delay)\n",
    "\n",
    "    # Create a gym-like environment to encapsulate the model\n",
    "    model_env = models.ModelEnv(env, dynamics_model, term_fn, reward_fn, generator=generator, n_delay=n_delay)\n",
    "    replay_buffer = common_util.create_replay_buffer(cfg, obs_shape, (act_shape[0]*(n_delay + 1), ), rng=rng, n_delay=n_delay)\n",
    "\n",
    "    common_util.rollout_agent_trajectories(\n",
    "        env,\n",
    "        model_env,\n",
    "        trial_length*50, # initial exploration steps\n",
    "        planning.RandomAgent(env),\n",
    "        {}, # keyword arguments to pass to agent.act()\n",
    "        replay_buffer=replay_buffer,\n",
    "        trial_length=trial_length\n",
    "    )\n",
    "\n",
    "    num_particles = 20\n",
    "    agent = planning.create_trajectory_optim_agent_for_model(\n",
    "        model_env,\n",
    "        agent_cfg,\n",
    "        num_particles=num_particles\n",
    "    )\n",
    "    agent2 = planning.create_trajectory_optim_agent_for_model(\n",
    "        model_env,\n",
    "        agent_cfg,\n",
    "        num_particles=num_particles\n",
    "    )\n",
    "\n",
    "    %matplotlib inline\n",
    "    import pickle\n",
    "    importlib.reload(models)\n",
    "    importlib.reload(cartpole_env_down)\n",
    "    importlib.reload(planning)\n",
    "    importlib.reload(common_util)\n",
    "\n",
    "    # Create a trainer for the model\n",
    "    model_trainer = models.ModelTrainer(dynamics_model, optim_lr=1e-3, weight_decay=5e-5)\n",
    "\n",
    "    # trajectory_renderer = TrajectoryRenderer(env.x_threshold, env.length, int(1 + np.ceil(50/5)))\n",
    "\n",
    "    # Create visualization objects\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 3.75), gridspec_kw={\"width_ratios\": [1, 1]})\n",
    "    ax_text = axs[0].text(300, 50, \"\")\n",
    "\n",
    "    trajectories_actions_all = []\n",
    "    trajectories_states_all = []\n",
    "    trajectories_actions_extra_all = []\n",
    "    obs_all = []\n",
    "    dreward_all = []\n",
    "    all_optimized_all = []\n",
    "    all_values_all = []\n",
    "    optim_traj_values_all = []\n",
    "    top_traj_values_all = []\n",
    "    extra_traj_optim_all = []\n",
    "    all_values_optim_all = []\n",
    "    extra_traj_all_optimized_all = []\n",
    "    extra_traj_values_all = []\n",
    "    errors_all = []\n",
    "    true_traj_all = []\n",
    "    values_before_all = []\n",
    "    # Main PETS loop\n",
    "    all_rewards = [0]\n",
    "    all_rewards2 = [0]\n",
    "    for trial in range(num_trials):\n",
    "        obs = env.reset()\n",
    "        env2.reset()\n",
    "        agent.reset()\n",
    "        agent2.reset()\n",
    "        model_env.reset_prev_actions()\n",
    "\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "        steps_trial = 0\n",
    "        trajectories_actions_trial = []\n",
    "        trajectories_actions_extra_trial = []\n",
    "        trajectories_states_trial = []\n",
    "        obs_trial = []\n",
    "        dreward_trial = []\n",
    "        all_optimized_trial = []\n",
    "        all_values_trial = []\n",
    "        optim_traj_values_trial = []\n",
    "        values_before_trial = []\n",
    "        top_traj_values_trial = []\n",
    "        extra_traj_optim_trial = []\n",
    "        all_values_optim_trial = []\n",
    "        extra_traj_all_optimized_trial = []\n",
    "        extra_traj_values_trial = []\n",
    "        error_trial = []\n",
    "        true_traj_trial = []\n",
    "        while not done:\n",
    "            env.render()\n",
    "            print(f'experiment: {experiment_number}, trial: {trial}. Step {steps_trial}')\n",
    "            # --------------- Model Training -----------------\n",
    "            if steps_trial == 0:\n",
    "                dynamics_model.update_normalizer(replay_buffer.get_all())  # update normalizer stats\n",
    "\n",
    "                dataset_train, dataset_val = replay_buffer.get_iterators(\n",
    "                    batch_size=cfg.overrides.model_batch_size,\n",
    "                    val_ratio=cfg.overrides.validation_ratio,\n",
    "                    train_ensemble=True,\n",
    "                    ensemble_size=ensemble_size,\n",
    "                    shuffle_each_epoch=True,\n",
    "                    bootstrap_permutes=False,  # build bootstrap dataset using sampling with replacement\n",
    "                )\n",
    "\n",
    "                model_trainer.train(\n",
    "                    dataset_train, dataset_val=dataset_val, num_epochs=50, patience=50, callback=train_callback)\n",
    "\n",
    "            true_traj_trial.append(obs)\n",
    "\n",
    "            # --- Doing env step using the agent and adding to model dataset ---\n",
    "            next_obs, reward, done, _ = common_util.step_env_and_add_to_buffer_optim(env, model_env, obs, agent, {}, replay_buffer, use_opt=True)\n",
    "\n",
    "            obs = next_obs\n",
    "            total_reward += reward\n",
    "            steps_trial += 1\n",
    "\n",
    "            if steps_trial == trial_length:\n",
    "                break\n",
    "\n",
    "        print('total reward', total_reward)\n",
    "\n",
    "    rew_all.append(all_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
