{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import omegaconf\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "os.environ['PATH'] += os.pathsep + os.pathsep.join([\"C:\\\\Users\\\\kehua\\\\.mujoco\\\\mujoco200\\\\bin;%PATH%\"])\n",
    "\n",
    "import mbrl.env.cartpole_continuous as cartpole_env\n",
    "import mbrl.env.cartpole_continuous_down as cartpole_env_down\n",
    "import mbrl.env.cartpole_continuous_travel as cartpole_env_travel\n",
    "import mbrl.env.pets_halfcheetah as half_cheetah\n",
    "import mbrl.env.pets_pusher as pusher\n",
    "import mbrl.env.pets_reacher as reacher\n",
    "import mbrl.env.humanoid_standup as humanoid_standup\n",
    "import mbrl.env.obstacles as obstacles\n",
    "\n",
    "import mbrl.env.reward_fns as reward_fns\n",
    "import mbrl.env.termination_fns as termination_fns\n",
    "import mbrl.models as models\n",
    "import mbrl.planning as planning\n",
    "import mbrl.util.common as common_util\n",
    "import mbrl.util as util\n",
    "from mbrl.types import TransitionBatch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import importlib\n",
    "importlib.reload(models)\n",
    "importlib.reload(cartpole_env_down)\n",
    "importlib.reload(planning)\n",
    "importlib.reload(common_util)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "mpl.rcParams.update({\"font.size\": 16})\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "env = cartpole_env_down.CartPoleEnvDown()\n",
    "env.seed(seed)\n",
    "env2 = cartpole_env_down.CartPoleEnvDown()\n",
    "env2.seed(seed)\n",
    "\n",
    "rng = np.random.default_rng(seed=0)\n",
    "generator = torch.Generator(device=device)\n",
    "generator.manual_seed(seed)\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape\n",
    "\n",
    "# This functions allows the model to evaluate the true rewards given an observation \n",
    "reward_fn = reward_fns.cartpole_down\n",
    "# This function allows the model to know if an observation should make the episode end\n",
    "term_fn = termination_fns.cartpole_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_length = 60\n",
    "num_trials = 8\n",
    "ensemble_size = 5\n",
    "n_delay = 1\n",
    "\n",
    "# Everything with \"???\" indicates an option with a missing value.\n",
    "# Our utility functions will fill in these details using the \n",
    "# environment information\n",
    "cfg_dict = {\n",
    "    # dynamics model configuration\n",
    "    \"dynamics_model\": {\n",
    "        \"model\": {\n",
    "            \"_target_\": \"mbrl.models.GaussianMLP\",\n",
    "            \"device\": device,\n",
    "            \"num_layers\": 3,\n",
    "            \"ensemble_size\": ensemble_size,\n",
    "            \"hid_size\": 200,\n",
    "            \"use_silu\": True,\n",
    "            \"in_size\": \"???\",\n",
    "            \"out_size\": \"???\",\n",
    "            \"deterministic\": False,\n",
    "            \"propagation_method\": \"fixed_model\"\n",
    "        }\n",
    "    },\n",
    "    # options for training the dynamics model\n",
    "    \"algorithm\": {\n",
    "        \"learned_rewards\": False,\n",
    "        \"target_is_delta\": True,\n",
    "        \"normalize\": True,\n",
    "    },\n",
    "    # these are experiment specific options\n",
    "    \"overrides\": {\n",
    "        \"trial_length\": trial_length,\n",
    "        \"num_steps\": num_trials * trial_length,\n",
    "        \"model_batch_size\": 32,\n",
    "        \"validation_ratio\": 0.05\n",
    "    }\n",
    "}\n",
    "cfg = omegaconf.OmegaConf.create(cfg_dict)\n",
    "\n",
    "agent_cfg = omegaconf.OmegaConf.create({\n",
    "    # this class evaluates many trajectories and picks the best one\n",
    "    \"_target_\": \"mbrl.planning.TrajectoryOptimizerAgent\",\n",
    "    \"planning_horizon\": 45,\n",
    "    \"replan_freq\": 1,\n",
    "    \"verbose\": False,\n",
    "    \"action_lb\": \"???\",\n",
    "    \"action_ub\": \"???\",\n",
    "    \"resample\": False,\n",
    "    # this is the optimizer to generate and choose a trajectory\n",
    "    \"optimizer_cfg\": {\n",
    "        \"_target_\": \"mbrl.planning.GradientOptimizer\",\n",
    "        \"num_iterations\": 5,\n",
    "        \"elite_ratio\": 0.1,\n",
    "        \"population_size\": 10,\n",
    "        \"alpha\": 0.1,\n",
    "        \"device\": device,\n",
    "        \"lower_bound\": \"???\",\n",
    "        \"upper_bound\": \"???\",\n",
    "        \"return_mean_elites\": True,\n",
    "        \"num_top\": 1,\n",
    "        \"resample_amount\": 10\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_scores = []\n",
    "\n",
    "def train_callback(_model, _total_calls, _epoch, tr_loss, val_score, _best_val):\n",
    "    train_losses.append(tr_loss)\n",
    "    val_scores.append(val_score.mean().item())   # this returns val score per ensemble model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dreward_total = {}\n",
    "for trial in range(num_trials):\n",
    "    dreward_total[trial] = []\n",
    "    \n",
    "values_total = {}\n",
    "for trial in range(num_trials):\n",
    "    values_total[trial] = []\n",
    "    \n",
    "values_prior = {}\n",
    "for trial in range(num_trials):\n",
    "    values_prior[trial] = []\n",
    "    \n",
    "values_total2 = {}\n",
    "for trial in range(num_trials):\n",
    "    values_total2[trial] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rew_all = []\n",
    "rew_all2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_experiments = 20\n",
    "for experiment_number in range(num_experiments):\n",
    "    # Create a 1-D dynamics model for this environme|nt\n",
    "    dynamics_model = common_util.create_one_dim_tr_model(cfg, obs_shape, act_shape, n_delay=n_delay)\n",
    "\n",
    "    # Create a gym-like environment to encapsulate the model\n",
    "    model_env = models.ModelEnv(env, dynamics_model, term_fn, reward_fn, generator=generator, n_delay=n_delay)\n",
    "    replay_buffer = common_util.create_replay_buffer(cfg, obs_shape, (act_shape[0]*(n_delay + 1), ), rng=rng, n_delay=n_delay)\n",
    "\n",
    "    common_util.rollout_agent_trajectories(\n",
    "        env,\n",
    "        model_env,\n",
    "        trial_length*50, # initial exploration steps\n",
    "        planning.RandomAgent(env),\n",
    "        {}, # keyword arguments to pass to agent.act()\n",
    "        replay_buffer=replay_buffer,\n",
    "        trial_length=trial_length\n",
    "    )\n",
    "\n",
    "    num_particles = 20\n",
    "    agent = planning.create_trajectory_optim_agent_for_model(\n",
    "        model_env,\n",
    "        agent_cfg,\n",
    "        num_particles=num_particles\n",
    "    )\n",
    "    agent2 = planning.create_trajectory_optim_agent_for_model(\n",
    "        model_env,\n",
    "        agent_cfg,\n",
    "        num_particles=num_particles\n",
    "    )\n",
    "\n",
    "    %matplotlib inline\n",
    "    import pickle\n",
    "    importlib.reload(models)\n",
    "    importlib.reload(cartpole_env_down)\n",
    "    importlib.reload(planning)\n",
    "    importlib.reload(common_util)\n",
    "\n",
    "    # Create a trainer for the model\n",
    "    model_trainer = models.ModelTrainer(dynamics_model, optim_lr=1e-3, weight_decay=5e-5)\n",
    "\n",
    "    # trajectory_renderer = TrajectoryRenderer(env.x_threshold, env.length, int(1 + np.ceil(50/5)))\n",
    "\n",
    "    # Create visualization objects\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 3.75), gridspec_kw={\"width_ratios\": [1, 1]})\n",
    "    ax_text = axs[0].text(300, 50, \"\")\n",
    "\n",
    "    trajectories_actions_all = []\n",
    "    trajectories_states_all = []\n",
    "    trajectories_actions_extra_all = []\n",
    "    obs_all = []\n",
    "    dreward_all = []\n",
    "    all_optimized_all = []\n",
    "    all_values_all = []\n",
    "    optim_traj_values_all = []\n",
    "    top_traj_values_all = []\n",
    "    extra_traj_optim_all = []\n",
    "    all_values_optim_all = []\n",
    "    extra_traj_all_optimized_all = []\n",
    "    extra_traj_values_all = []\n",
    "    errors_all = []\n",
    "    true_traj_all = []\n",
    "    values_before_all = []\n",
    "    # Main PETS loop\n",
    "    all_rewards = [0]\n",
    "    all_rewards2 = [0]\n",
    "    for trial in range(num_trials):\n",
    "        obs = env.reset()\n",
    "        env2.reset()\n",
    "        agent.reset()\n",
    "        agent2.reset()\n",
    "        model_env.reset_prev_actions()\n",
    "\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "        steps_trial = 0\n",
    "        trajectories_actions_trial = []\n",
    "        trajectories_actions_extra_trial = []\n",
    "        trajectories_states_trial = []\n",
    "        obs_trial = []\n",
    "        dreward_trial = []\n",
    "        all_optimized_trial = []\n",
    "        all_values_trial = []\n",
    "        optim_traj_values_trial = []\n",
    "        values_before_trial = []\n",
    "        top_traj_values_trial = []\n",
    "        extra_traj_optim_trial = []\n",
    "        all_values_optim_trial = []\n",
    "        extra_traj_all_optimized_trial = []\n",
    "        extra_traj_values_trial = []\n",
    "        error_trial = []\n",
    "        true_traj_trial = []\n",
    "        while not done:\n",
    "            env.render()\n",
    "            print(f'experiment: {experiment_number}, trial: {trial}. Step {steps_trial}')\n",
    "            # --------------- Model Training -----------------\n",
    "            if steps_trial == 0:\n",
    "                dynamics_model.update_normalizer(replay_buffer.get_all())  # update normalizer stats\n",
    "\n",
    "                dataset_train, dataset_val = replay_buffer.get_iterators(\n",
    "                    batch_size=cfg.overrides.model_batch_size,\n",
    "                    val_ratio=cfg.overrides.validation_ratio,\n",
    "                    train_ensemble=True,\n",
    "                    ensemble_size=ensemble_size,\n",
    "                    shuffle_each_epoch=True,\n",
    "                    bootstrap_permutes=False,  # build bootstrap dataset using sampling with replacement\n",
    "                )\n",
    "\n",
    "                model_trainer.train(\n",
    "                    dataset_train, dataset_val=dataset_val, num_epochs=50, patience=50, callback=train_callback)\n",
    "\n",
    "            true_traj_trial.append(obs)\n",
    "\n",
    "            # --- Doing env step using the agent and adding to model dataset ---\n",
    "            next_obs, reward, done, _ = common_util.step_env_and_add_to_buffer_optim(env, model_env, obs, agent, {}, replay_buffer, use_opt=True)\n",
    "\n",
    "            obs = next_obs\n",
    "            total_reward += reward\n",
    "            steps_trial += 1\n",
    "\n",
    "            if steps_trial == trial_length:\n",
    "                break\n",
    "\n",
    "        print('total reward', total_reward)\n",
    "\n",
    "    rew_all.append(all_rewards)\n",
    "    \n",
    "    # Save results to plot\n",
    "    with open(f'results/cartpole_swingup/500_samples_CEM', 'wb+') as file:\n",
    "        pickle.dump(rew_all, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
